{"cells":[{"cell_type":"markdown","id":"5117bd05","metadata":{"id":"5117bd05"},"source":["이 계층들을 조합하여 손글씨 숫자를 인식하는 CNN을 조립해보자. 여기에서는 다음과 같은 CNN을 구현한다.\n","\n","<img src=images/7_23.png height=100px width=500px>\n","\n","이 그림의 CNN 네트워크는 \"Convolution-ReLU-Pooling-Affine-ReLU-Affine-Softmax\"순으로 흐른다. 이를 SimpleConvNet이라는 이름의 클래스로 구현하자.\n","\n","우선 SimpleConvNet의 초기화 (init)을 살펴보자. 초기화 때는 다음 인수들을 받는다.\n","\n","- input_dim - 입력 데이터(채널 수, 높이, 너비)의 차원\n","- conv_param - 합성곱 계층의 하이퍼파라미터(딕셔너리). 딕셔너리의 키는 다음과 같다.\n","    - filter_num - 필터수\n","    - filter_size - 필터 사이즈\n","    - stride - 스트라이드\n","    - pad - 패딩\n","- hidden_size - 은닉층(완전연결)의 뉴런 수\n","- output_size - 출력층(완전연결)의 뉴런 수\n","- weight_init_std - 초기화 때의 가중치 표준편차\n","\n","여기에서 합성곱 계층의 하이퍼파라미터는 딕셔너리 형태로 주어진다. (conv-param). 이것은 필요한 하이퍼파라미터의 값이 예컨대 {\"filter_num\": 30, \"filter_size\": 5, \"pad\": 0, \"stride\": 1}처럼 저장된다는 뜻이다. "]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks/Deep_Learning\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmqfsUBHkzu-","executionInfo":{"status":"ok","timestamp":1646125015636,"user_tz":-540,"elapsed":4437,"user":{"displayName":"이도훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY2UtQYAONZ3e2VPWo99yHVjRD2S32OBLGcSNB1w=s64","userId":"08244818386454187914"}},"outputId":"eea3c976-aa16-4c8b-d44a-63f29f64f7f9"},"id":"YmqfsUBHkzu-","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"id":"17deaf70","metadata":{"id":"17deaf70","executionInfo":{"status":"ok","timestamp":1646125023162,"user_tz":-540,"elapsed":320,"user":{"displayName":"이도훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY2UtQYAONZ3e2VPWo99yHVjRD2S32OBLGcSNB1w=s64","userId":"08244818386454187914"}}},"outputs":[],"source":["import numpy as np\n","from collections import OrderedDict\n","from Layers import *\n","\n","class SimpleConvNet:\n","    def __init__(self, input_dim=(1,28,28),\n","                conv_param={\"filter_num\": 30, \"filter_size\": 5, \"pad\": 0, \"stride\": 1},\n","                hidden_size=100, output_size=10, weight_init_std=0.01):\n","        ## part 1\n","        filter_num = conv_param[\"filter_num\"]\n","        filter_size = conv_param[\"filter_size\"]\n","        filter_pad = conv_param[\"pad\"]\n","        filter_stride = conv_param[\"stride\"]\n","        input_size = input_dim[1]\n","        conv_output_size = (input_size - filter_size + 2*filter_pad)/filter_stride + 1\n","        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n","        \n","        ## part 2\n","        self.params = {}\n","        self.params[\"W1\"] = weight_init_std * \\\n","                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n","        self.params[\"b1\"] = np.zeros(filter_num)\n","        self.params[\"W2\"] = weight_init_std * \\\n","                            np.random.randn(filter_num, pool_output_size, filter_size, filter_size)\n","        self.params[\"b2\"] = np.zeros(hidden_size)\n","        self.params[\"W3\"] = weight_init_std * \\\n","                            np.random.randn(hidden_size, output_size)\n","        self.params[\"b3\"] = np.zeros(output_size)\n","        \n","        ## part 3\n","        self.layers = OrderedDict()\n","        self.layers[\"Conv1\"] = Convolution(self.params[\"W1\"],\n","                                          self.params[\"b1\"],\n","                                          conv_param[\"stride\"],\n","                                          conv_param[\"pad\"])\n","        self.layers[\"ReLU1\"] = ReLULayer()\n","        self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_w=2, stride=2)\n","        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n","        self.layers[\"ReLU2\"] = ReLULayer()\n","        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"b3\"])\n","        \n","        self.last_layer = SoftmaxLossLayer()\n","        \n","        ## part 4\n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","            \n","        return x\n","    \n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        loss = self.last_layer.forward(y, t)\n","        \n","        return loss\n","    \n","    def accuracy(self, x, t, batch_size=100):\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","            \n","        acc = 0.0\n","        \n","        for i in range(int(x.shape[0] / batch_size)):\n","            tx = x[i*batch_size:(i+1)*batch_size]\n","            tt = t[i*batch_size:(i+1)*batch_size]\n","            y = self.predict(y, axis=1)\n","            y = np.argmax(y, axis=1)\n","            acc += np.sum(y == tt)\n","            \n","        return acc / x.shape[0]\n","        \n","        ## part 5\n","    def gradient(self, x, t):\n","        self.loss(x, t)\n","        \n","        dout = 1\n","        dout = self.last_layer.backward(dout)\n","        \n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","            \n","        grads = {}\n","        grads[\"W1\"], grads[\"b1\"] = self.layers[\"Conv1\"].dW, self.layers[\"Conv1\"].db\n","        grads[\"W2\"], grads[\"b2\"] = self.layers[\"Affine1\"].dW, self.layers[\"Affine1\"].db\n","        grads[\"W3\"], grads[\"b3\"] = self.layers[\"Affine2\"].dW, self.layers[\"Affine2\"].db\n","        \n","        return grads"]},{"cell_type":"markdown","id":"e75b7de1","metadata":{"id":"e75b7de1"},"source":["##### part 1\n","\n","여기에서는 초기화 인수로 주어진 합성곱 계층의 하이퍼파라미터를 딕셔너리에서 꺼낸다(나중에 쓰기 쉽도록). 그리고 합성곱 계층의 출력 크기를 계산한다.\n","\n","##### part 2\n","\n","학습에 필요한 매개변수는 1번째 층의 합성곱 계층과 나머지 두 완전연결 계층의 가중치와 편향이다. 이 매개변수들을 인스턴스 변수 params 딕셔너리에 저장한다. 1번째 층의 합성곱 계층의 가중치를 W1, 편향을 b1이라는 키로 저장한다. 마찬가지로 2번째 층의 완전연결 계층의 가중치와 편향을 W2와 b2, 마지막 3번째 층의 완전연결 계층의 가중치와 편향을 W3와 b3라는 키로 각각 저장한다.\n","\n","##### part 3\n","\n","순서가 있는 $딕셔너리^{OrderedDict}$인 layers에 계층들을 차례로 추가한다. 마지막 SoftmaxWithLoss 계층만큼은 last_layer라는 별고 변수에 저장해둔다.\n","\n","##### part 4\n","\n","이상이 SimpleConvNet의 초기화이다. 이렇게 초기화를 마친 다음에는 추론을 수행하는 predict 메소드와 손실 함수의 값을 구하는 loss 메소드를 다음과 같이 구현할 수 있다.\n","\n","##### summarize\n","이 코드에서 인수 x는 입력 데이터, t는 정답 레이블이다. 추론을 수행하는 predict 메소드는 초기화 때 layers에 추가한 계층을 맨 앞에서부터 차례로 forward 메소드를 호출하며 그 결과를 다음 계층에 전달한다. 손실 함수를 구하는 loss 메소드는 predict 메소드의 결과를 인수로 마지막 층의 forward 메소드를 호출한다. 즉, 첫 계층부터 마지막 계층까지 forward를 처리한다.\n","\n","##### part 5\n","\n","매개변수의 기울기는 오차역전파법으로 구한다. 이 과정은 순전파와 역전파를 반복한다. 지금까지 각 계층의 순전파와 역전파 기능을 제대로 구현했다면, 여기에서는 단지 그것들을 적절한 순서로 호출만 된다. 마지막으로 grads라는 딕셔너리 변수에 각 가중치 매개변수 의 기울기를 저장한다. 이상이 SimpleConvNet의 구현이다. "]},{"cell_type":"markdown","id":"c866df38","metadata":{"id":"c866df38"},"source":["이제 이 SimpleConvNet으로 MNIST 데이터셋을 학습해볼 차례이다."]},{"cell_type":"code","execution_count":3,"id":"73df5fab","metadata":{"id":"73df5fab","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"error","timestamp":1646125625822,"user_tz":-540,"elapsed":15655,"user":{"displayName":"이도훈","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiY2UtQYAONZ3e2VPWo99yHVjRD2S32OBLGcSNB1w=s64","userId":"08244818386454187914"}},"outputId":"c50bcdcf-99ed-44b2-85aa-f7b40a7c3f72"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6c485650f154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/Deep_Learning/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/Deep_Learning/simple_convnet.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, x, t, batch_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mtx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/Deep_Learning/simple_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/Deep_Learning/Layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;31m## 전개\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_h\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/Colab Notebooks/Deep_Learning/functions.py\u001b[0m in \u001b[0;36mim2col\u001b[0;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import sys, os\n","sys.path.append(os.pardir)\n","from simple_convnet import SimpleConvNet\n","from optimizer import SGD\n","from trainer import Trainer\n","from source.dataset.mnist import load_mnist\n","import matplotlib.pyplot as plt\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False, one_hot_label=True)\n","\n","x_train, t_train = x_train[:5000], t_train[:5000]\n","x_test, t_test = x_test[:1000], t_test[:1000]\n","\n","network = SimpleConvNet(input_dim=(1,28,28),\n","                        conv_param={\"filter_num\": 30, \"filter_size\": 5, \"pad\": 0, \"stride\": 1},\n","                        hidden_size=100, output_size=10, weight_init_std=0.01)\n","\n","trainer = Trainer(network, x_train, t_train, x_test, t_test)\n","\n","trainer.train()"]},{"cell_type":"code","source":[""],"metadata":{"id":"LrAjOVRUsMau"},"id":"LrAjOVRUsMau","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"7.5_implement_CNN.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}